{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49b02f4e-c658-4d24-b31e-4d6da33e548b",
   "metadata": {},
   "source": [
    "Q1. What is an ensemble technique in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "564f7477-83fd-446f-b435-ea52896db579",
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are several popular ensemble techniques, including:\n",
    "\n",
    "#Bagging (Bootstrap Aggregating): Bagging involves training multiple instances of the same base model on different subsets of the training data. These subsets are typically\n",
    "#created by randomly sampling the data with replacement (bootstrap samples). The final prediction is often obtained by averaging (for regression) or taking a majority vote\n",
    "#(for classification) of the predictions from the individual models. Random Forests are a well-known example of a bagging ensemble technique.\n",
    "#\n",
    "#Boosting: Boosting is an ensemble technique where models are trained sequentially, and each new model focuses on the examples that the previous ones struggled with.\n",
    "#Common boosting algorithms include AdaBoost, Gradient Boosting, and XGBoost.\n",
    "\n",
    "#Stacking: Stacking combines multiple base models by training a meta-model on their predictions. Instead of using simple averaging or voting, stacking learns how to best \n",
    "#combine the predictions of the base models. It often involves cross-validation to prevent overfitting.\n",
    "\n",
    "#Voting: Voting ensembles combine the predictions of multiple models by taking a majority vote (for classification) or averaging (for regression). There are three main\n",
    "#types of voting ensembles: hard voting, soft voting, and weighted voting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b732e37d-2366-4f28-b2d7-c94c53826361",
   "metadata": {},
   "source": [
    "Q2. Why are ensemble techniques used in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce2a6673-4e31-4d0b-9d5e-7cdc21808601",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Improved Accuracy: One of the primary motivations for using ensemble techniques is to improve the predictive accuracy of machine learning models. By combining the\n",
    "#predictions of multiple models, ensembles can often provide more accurate results than any individual model. This is especially beneficial when dealing with complex \n",
    "#or noisy data.\n",
    "\n",
    "#Reduced Overfitting: Ensembles tend to be more robust against overfitting compared to single models. Overfitting occurs when a model learns to fit the training data\n",
    "#too closely, capturing noise rather than true patterns. Ensemble methods, such as bagging and boosting, involve combining multiple base models, which can help reduce\n",
    "#overfitting and improve generalization to unseen data.\n",
    "\n",
    "#Increased Robustness: Ensembles are less sensitive to outliers and anomalies in the data. Outliers may have a disproportionate impact on single models, but when\n",
    "#multiple models are combined, their influence is diluted. This makes ensembles more robust in scenarios where data quality is a concern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978f5123-241c-4e8a-a658-b1999163c7c0",
   "metadata": {},
   "source": [
    "Q3. What is bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7af1dcfe-8b5b-4124-88b5-d4f4f2fe11ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bagging, which stands for Bootstrap Aggregating, is an ensemble machine learning technique used to improve the accuracy and robustness of predictive models. It works \n",
    "# by training multiple instances of the same base model on different subsets of the training data and then combining their predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0bc01a-3c0a-48ac-9cb9-da97cec11348",
   "metadata": {},
   "source": [
    "Q4. What is boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6987c00f-bf78-4513-b599-70b784838e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boosting is an ensemble machine learning technique that aims to improve the performance of weak or base models by combining them in a sequential manner. Unlike \n",
    "#bagging, where base models are trained independently, boosting involves training base models iteratively, with each subsequent model focusing on the examples that \n",
    "#the previous ones struggled with. The primary goal of boosting is to create a strong predictive model by giving more weight to difficult-to-predict instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715c5026-3d8a-4b0e-a5ef-b9bb668181a2",
   "metadata": {},
   "source": [
    "Q5. What are the benefits of using ensemble techniques?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "341b0144-9810-48d9-bc87-fecc6c6f3d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved Accuracy: Ensembles can significantly boost predictive accuracy by combining multiple models. They often outperform individual models, especially when \n",
    "# those models are prone to bias or overfitting.\n",
    "\n",
    "#Reduced Overfitting: Ensembles are less likely to overfit the training data compared to single models. By combining the predictions of multiple models that have been \n",
    "# trained on different subsets of data or with different algorithms, ensembles can mitigate the risk of overfitting.\n",
    "\n",
    "# Robustness: Ensembles are more robust to noise and outliers in the data. Individual models may make incorrect predictions on certain data points due to noise, but \n",
    "# ensembles can smooth out these errors by aggregating multiple predictions.\n",
    "\n",
    "#Increased Generalization: Ensemble techniques often lead to better generalization to unseen data. They capture a broader range of patterns and relationships in the data,\n",
    "#which can improve the model's ability to make accurate predictions on new, unseen examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7eea76-27a9-4175-8446-d200d6b94218",
   "metadata": {},
   "source": [
    "Q6. Are ensemble techniques always better than individual models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b93eaf4a-2c0f-4171-9307-8eb6a14c52ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Complexity: Ensembles add complexity to the modeling process. They require training and combining multiple models, which can be computationally expensive and may not be \n",
    "#necessary for relatively simple tasks. In such cases, a single well-tuned model might perform adequately without the need for ensembling.\n",
    "\n",
    "#Data Availability: If you have a small or limited dataset, ensembles may not always be beneficial. Ensembles thrive when there is diversity among base models, which is \n",
    "# easier to achieve with larger datasets. With limited data, it may be challenging to train diverse models effectively.\n",
    "\n",
    "#Time and Resources: Building and maintaining an ensemble requires more time and resources than training a single model. For time-sensitive applications or \n",
    "#resource-constrained environments, the overhead of ensembling might not be justified.\n",
    "\n",
    "#Overfitting Risk: While ensembles can reduce overfitting, they are not immune to it. If you create a very complex ensemble with numerous models or if you continue to add\n",
    "# models without proper tuning, you can still overfit the ensemble to the training data.\n",
    "\n",
    "#Interpretability: Ensembles can be less interpretable than individual models. Combining predictions from multiple models can make it harder to understand how the final \n",
    "# decision was reached. If interpretability is a critical requirement, using a single model might be preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e23c249-fb0e-4779-a0cb-733ceabfb37c",
   "metadata": {},
   "source": [
    "Q7. How is the confidence interval calculated using bootstrap?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7844f273-4526-4861-bb04-8917cf6aa38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here are the general steps for calculating a bootstrap confidence interval:\n",
    "\n",
    "#Data Resampling: Start with your original dataset, which has 'n' data points. Create a large number of resampled datasets by randomly selecting 'n' data points from\n",
    "#the original data with replacement. This means that some data points may appear multiple times in a resampled dataset, while others may not appear at all.\n",
    "\n",
    "#Statistic Calculation: For each resampled dataset, calculate the statistic of interest. For example, if you want to estimate the mean, calculate the mean for each \n",
    "#resampled dataset.\n",
    "#\n",
    "#Sampling Distribution: You will now have a collection of statistics (e.g., means) from the resampled datasets. This collection forms the sampling distribution of the\n",
    "#statistic. The sampling distribution represents the possible values of the statistic that could be observed if you were to draw many random samples from the \n",
    "#population (with replacement)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aec275-04ba-419c-af75-18b75d5a645b",
   "metadata": {},
   "source": [
    "Q8. How does bootstrap work and What are the steps involved in bootstrap?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "763ae0a3-4d00-4c55-92cc-5ba9c0c9fc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original Dataset: Begin with your original dataset, which contains 'n' observations or data points.\n",
    "\n",
    "#Resampling: Randomly select 'n' data points from the original dataset with replacement to create a bootstrap sample. Since you sample with replacement, some data points \n",
    "#may appear multiple times in a bootstrap sample, while others may not appear at all.\n",
    "\n",
    "#Statistic Calculation: Calculate the statistic of interest for the bootstrap sample. The statistic could be a mean, median, standard deviation, variance, confidence \n",
    "#interval, or any other measure you want to estimate or analyze.\n",
    "\n",
    "#Repeat: Repeat steps 2 and 3 a large number of times (typically thousands or more) to generate a collection of bootstrap statistics. Each iteration of this process produces \n",
    "#a new bootstrap sample and a new value for the statistic.\n",
    "\n",
    "#Sampling Distribution: The collection of statistics obtained from the repeated resampling forms the bootstrap sampling distribution of the statistic. This distribution \n",
    "#represents the variability in the statistic that you would observe if you were to repeatedly draw random samples from the same population (with replacement)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5552ecba-74a0-40be-9bc8-162b005d67e7",
   "metadata": {},
   "source": [
    "Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a \n",
    "sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use \n",
    "bootstrap to estimate the 95% confidence interval for the population mean height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b159e629-65b3-4401-913c-481a3be15d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% Confidence Interval for Mean Height (meters): [15. 15.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "original_data = np.array([15] * 50)  \n",
    "num_bootstrap_samples = 10000\n",
    "bootstrap_means = np.zeros(num_bootstrap_samples)\n",
    "\n",
    "for i in range(num_bootstrap_samples):\n",
    "   \n",
    "    bootstrap_sample = np.random.choice(original_data, size=len(original_data), replace=True)\n",
    "    \n",
    "    bootstrap_means[i] = np.mean(bootstrap_sample)\n",
    "\n",
    "confidence_interval = np.percentile(bootstrap_means, [2.5, 97.5])\n",
    "\n",
    "print(\"95% Confidence Interval for Mean Height (meters):\", confidence_interval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e137c2-dc22-4f7d-a87e-694939a481ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
